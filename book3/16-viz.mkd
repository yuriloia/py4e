Visualização de dados 
================

Até o momento nós estávamos aprendendo a linguagem Python para depois aprendermos a como utilizar Python, 
as redes de computadores e banco de dados para manipulação de dados.

Neste capítulo, nós analisaremos três aplicações que reúnem todas esses conceitos para gerir e vizualizar dados. Você poderá usar essas aplicações como um código base para tea ajudar a começar a resolver um problema do mundo real. 

Todas essas aplicações estão num arquivo ZIP que você pode fazer o dowload e extrair no seu computador.

Construindo um mapa do Google a partir de dados geocodificados
----------------------------------------

\index{Google!mapa}
\index{Visualização!mapa}

Neste projeto, nós usaremos a API do Google de Geocodificação para organizar algumas informações de geográficas de localização de nomes de universidades que foram disponibilizados por usuários, para então colocar esses dados em um Google map.


![A Google Map](../images/google-map)
Para começar faça o download da aplicação em:

[www.py4e.com/code3/geodata.zip](http://www.py4e.com/code3/geodata.zip)

O primeiro problema a ser resolvido é que a API de geocodificação gratuita do Google é
taxa limitada a um determinado número de solicitações por dia. Se você tem muito
de dados, pode ser necessário parar e reiniciar o processo de pesquisa várias
vezes. Então por isso dividiremos o problema em duas fases.

\index{cache}

Na primeira fase, utilizam-se os dados de "pesquisa" de entrada no arquivo
*where.data* e lê-se uma linha de cada vez para recuperar
as informações geocodificadas do Google e as armazenam em um banco de dados
*geodata.sqlite*. Antes de usarmos a API de geocodificação para
cada informação inserida pelo usuário, nós verificamos se já temosos dados dessa linha de entrada específica. 
O banco de dados está funcionando
como um "cache" local dos nossos dados de geocodificação para garantir que nunca perguntemos
Google para os mesmos dados duas vezes.

Você pode reiniciar o processo a qualquer momento removendo o arquivo
*geodata.sqlite*.

Rode o program *geoload.py*. Este programa lerá as linhas de entrada em *where.data* e para cada linha ele checará se elas já estão no banco de dados. Se não os dados não existirem para o local analisado, a API de geocodificação será chamada para recuperar o dado e guardar no banco de dados.

Aqui está o uma amostra de execução após já existir dados armazenados no banco de dados:

~~~~
Found in database  Northeastern University
Found in database  University of Hong Kong, ...
Found in database  Technion
Found in database  Viswakarma Institute, Pune, India
Found in database  UMD
Found in database  Tufts University

Resolving Monash University
Retrieving http://maps.googleapis.com/maps/api/
    geocode/json?address=Monash+University
Retrieved 2063 characters {    "results" : [
{'status': 'OK', 'results': ... }

Resolving Kokshetau Institute of Economics and Management
Retrieving http://maps.googleapis.com/maps/api/
    geocode/json?address=Kokshetau+Inst ...
Retrieved 1749 characters {    "results" : [
{'status': 'OK', 'results': ... }
...
~~~~

Os cinco primeiro lugares já estão no banco de dados e portanto, eles são pulados. O programa escaneia o arquivo até o ponto em que ele encontra novos lugares e começa a recuperá-los.

O programa *geoload.py* pode ser parado a qualquer momento, além disso, existe um contador que você pode utilizar para limitar o número de vezes que se é chamada a API de geocodificação a cada vez que se é executado o programa. Dado que *where.data* apenas possui alguma centena de itens de dados, sua execução não deverá ultrapassar o limite diário, entretanto, se você tiver possuir mais dados, pode ser que sejam necessárias várias execuções ao longo de vários dias para que se consiga extrair os dados geocodificados.

Assim que possuir seus dados carregados em *geodata.sqlite*, poderá visualizar os dados usando o programa *geodump.py*. Este programa lê um banco de dados e armazena no arquivo *where.js* a localização, latitude e longitude na forma de um código em Javascript executável.

A execução do programa *geodump.py* é mostrada a seguir:

~~~~
Northeastern University, ... Boston, MA 02115, USA 42.3396998 -71.08975
Bradley University, 1501 ... Peoria, IL 61625, USA 40.6963857 -89.6160811
...
Technion, Viazman 87, Kesalsaba, 32000, Israel 32.7775 35.0216667
Monash University Clayton ... VIC 3800, Australia -37.9152113 145.134682
Kokshetau, Kazakhstan 53.2833333 69.3833333
...
12 records written to where.js
Open where.html to view the data in a browser
~~~~

O arquivo *where.html* consiste em um programa escrito em Html e Javascript para visualizar um mapa do Google. Ele lê os dados mais recentes em *where.js* para fazer com que os dados sejam visualizados. Aqui está o formato to arquivo do *where.js*:

~~~~ {.js}
myData = [
[42.3396998,-71.08975, 'Northeastern Uni ... Boston, MA 02115'],
[40.6963857,-89.6160811, 'Bradley University, ... Peoria, IL 61625, USA'],
[32.7775,35.0216667, 'Technion, Viazman 87, Kesalsaba, 32000, Israel'],
   ...
];
~~~~

Esta é uma variável em Javascript que contém uma lista de listas. A sintáxe para uma lista constante nesta linguagem é muito semelhante a lista em Python, logo esta sintáxe será familiar para você.

Simplismente abra o arquivo *where.html* no seu navegador e veja as localizações. Você poderá se aproximar de cada um dos pins para descrobrir a localização que a API de geocodificação retornará a entrada fornecida pelo usuário. Caso você não consiga ver nenhum dado quando abrir o arquivo *where.html*, você poderá checar o Javascript ou o console de desenvolvimento do seu navegador.

Vizualização de redes e interconexões
-----------------------------------------

\index{Google!page rank}
\index{Vizualização!redes}
\index{Vizualização!page rank}

Nesta aplicação, executaremos algumas das funções de uma ferramenta de busca. Primeiro, criaremos um pequeno subconjunto da web e executaremos um versão simplificada do algoritmo de classificação do Google para determinar quais as páginas estão mais altamente conectadas, para assim visualizar o page rank e a conectividade do nosso pequeno canto da web. Vamos usar o D3 Biblioteca de visualização JavaScript <http://d3js.org/> para produzir a saída de visualização.


Você pode fazer o download e extrair essa aplicação do site:

[www.py4e.com/code3/pagerank.zip](http://www.py4e.com/code3/pagerank.zip)

![A Page Ranking](height=3.5in@../images/pagerank)

O primeiro programa (*spider.py*) rastreia um site
e armazena uma série de páginas para o banco de dados
(*spider.sqlite*), gravando os links entre as páginas. Você
pode reiniciar o processo a qualquer momento removendo o
arquivo *spider.sqlite* e realizando uma nova execução do programa *spider.py*.


~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:2
1 http://www.dr-chuck.com/ 12
2 http://www.dr-chuck.com/csev-blog/ 57
How many pages:
~~~~

Nesta amostra de execução, pedimos para rastrear um site e recuperar duas páginas. Se você reiniciar o programa e solicitar que ele rastreie mais páginas, não rastreará nenhuma página que já esteja no banco de dados. Ao reiniciá-lo, ele irá para uma página aleatória não rastreada e começará o rastreio apartir daí. Então, cada execução do programa *spider.py* é aditiva.

~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:3
3 http://www.dr-chuck.com/csev-blog 57
4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1
5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13
How many pages:
~~~~

Você pode ter vários pontos de partida no mesmo banco de dados (dentro do
programa), eles são chamados de "webs"(teias em português). A aranha escolhe aleatoriamente entre
todos os links não visitados em todas as webs como a próxima página para rastrear.

Se você deseja jogar fora  o conteúdo do *spider.sqlite*, você pode executar o programa *spdump.py* da seguinte maneira:

~~~~
(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')
(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

This shows the number of incoming links, the old page rank, the new page
rank, the id of the page, and the url of the page. The
*spdump.py* program only shows pages that have at least
one incoming link to them.

Once you have a few pages in the database, you can run page rank on the
pages using the *sprank.py* program. You simply tell it
how many page rank iterations to run.

~~~~
How many iterations:2
1 0.546848992536
2 0.226714939664
[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]
~~~~

Você pode remover o banco de dados de novo para confirmar que o page rank foi atualizado.

~~~~
(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')
(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

Você pode executar o programa *sprank.py* quantas vezes quiser e ele
simplesmente refinará a classificação da página a cada execução. Você pode até rodar o programa *sprank.py* algumas vezes e depois rastrear mais algumas páginas com o programa *spider.py*, para assim executar o *sprank.py* para reconvergir os valores de classificação da página. Um motor de busca geralmente executa os programas de rastreamento e classificação o tempo todo.

Se você deseja reiniciar os cálculos de classificação da página sem considerar
nas páginas da web, você pode usar o programa *spreset.py* e assim reiniciar o programa 
*sprank.py*.


~~~~
How many iterations:50
1 0.546848992536
2 0.226714939664
3 0.0659516187242
4 0.0244199333
5 0.0102096489546
6 0.00610244329379
...
42 0.000109076928206
43 9.91987599002e-05
44 9.02151706798e-05
45 8.20451504471e-05
46 7.46150183837e-05
47 6.7857770908e-05
48 6.17124694224e-05
49 5.61236959327e-05
50 5.10410499467e-05
[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]
~~~~

Para cada iteração do algoritmo de page rank, é mostrado na tela a média de
mudança no page rank por página. A rede inicialmente é bastante desequilibrada
e, portanto, os valores individuais de classificação da página mudam bastante entre as iterações.
No entando, em algumas poucas iterações, o page rank converge. Você deveria rodar o programa
*sprank.py* tempo suficiente para que os valores de page rank convirjam.

Se você deseja visualizar as principais páginas em termos de classificação da página,
execute o programa *spjson.py* para ler o banco de dados e gravar os dados no formato JSON,
para que as páginas mais altamente vinculadas sejam visualizadas em um navegador web.


~~~~
Creating JSON output on spider.json...
How many nodes? 30
Open force.html in a browser to view the visualization
~~~~

Você pode visualizar esses dados abrindo o arquivo *force.html*
no seu navegador da web. Isso mostrará um layout automático dos nós e dos links. Você pode clicar e arrastar qualquer nó e também clicar duas vezes em um nó para encontrar a URL representadado por ele.

Se você executar novamente os outros utilitários, execute novamente o programa *spjson.py* e
pressione atualizar no navegador para obter os novos dados de do arquivo *spider.json*.



Visualizing mail data
---------------------

Up to this point in the book, you have become quite familiar with our
*mbox-short.txt* and *mbox.txt* data
files. Now it is time to take our analysis of email data to the next
level.

In the real world, sometimes you have to pull down mail data from
servers. That might take quite some time and the data might be
inconsistent, error-filled, and need a lot of cleanup or adjustment. In
this section, we work with an application that is the most complex so
far and pull down nearly a gigabyte of data and visualize it.

![A Word Cloud from the Sakai Developer List](height=3.5in@../images/wordcloud)

You can download this application from:

[www.py4e.com/code3/gmane.zip](http://www.py4e.com/code3/gmane.zip)

We will be using data from a free email list archiving service called
[www.gmane.org](http://www.gmane.org). This service is very popular with open
source projects because it provides a nice searchable archive of their
email activity. They also have a very liberal policy regarding accessing
their data through their API. They have no rate limits, but ask that you
don't overload their service and take only the data you need. You can
read gmane's terms and conditions at this page:

<http://gmane.org/export.php>

*It is very important that you make use of the gmane.org data
responsibly by adding delays to your access of their services and
spreading long-running jobs over a longer period of time. Do not abuse
this free service and ruin it for the rest of us.*

When the Sakai email data was spidered using this software, it produced
nearly a Gigabyte of data and took a number of runs on several days. The
file *README.txt* in the above ZIP may have instructions
as to how you can download a pre-spidered copy of the
*content.sqlite* file for a majority of the Sakai email
corpus so you don't have to spider for five days just to run the
programs. If you download the pre-spidered content, you should still run
the spidering process to catch up with more recent messages.

O primeiro passo é rastrar o repositório gmane. A URL base é codificado rigidamente no código em *gmane.py*e na lista de desenvolvedores Sakai. Você também pode rastrear outro repositório alterando o url base. Tenha certeza de deletar o arquivo *content.sqlite* caso você altere o url.
 

The *gmane.py* file operates as a responsible caching
spider in that it runs slowly and retrieves one mail message per second
so as to avoid getting throttled by gmane. It stores all of its data in
a database and can be interrupted and restarted as often as needed. It
may take many hours to pull all the data down. So you may need to
restart several times.

Aqui está uma execução do arquivo *gmane.py* retornando as últimas cinco mensagens da lista de desenvolvedores de Sakai

~~~~
How many messages:10
http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

Does not start with From
~~~~

The program scans *content.sqlite* from one up to the
first message number not already spidered and starts spidering at that
message. It continues spidering until it has spidered the desired number
of messages or it reaches a page that does not appear to be a properly
formatted message.

Sometimes [gmane.org](gmane.org) is missing a message. Perhaps
administrators can delete messages or perhaps they get lost. If your
spider stops, and it seems it has hit a missing message, go into the
SQLite Manager and add a row with the missing id leaving all the other
fields blank and restart *gmane.py*. This will unstick
the spidering process and allow it to continue. These empty messages
will be ignored in the next phase of the process.

One nice thing is that once you have spidered all of the messages and
have them in *content.sqlite*, you can run
*gmane.py* again to get new messages as they are sent to
the list.

The *content.sqlite* data is pretty raw, with an
inefficient data model, and not compressed. This is intentional as it
allows you to look at *content.sqlite* in the SQLite
Manager to debug problems with the spidering process. It would be a bad
idea to run any queries against this database, as they would be quite
slow.

The second process is to run the program *gmodel.py*.
This program reads the raw data from *content.sqlite* and
produces a cleaned-up and well-modeled version of the data in the file
*index.sqlite*. This file will be much smaller (often 10X
smaller) than *content.sqlite* because it also compresses
the header and body text.

Each time *gmodel.py* runs it deletes and rebuilds
*index.sqlite*, allowing you to adjust its parameters and
edit the mapping tables in *content.sqlite* to tweak the
data cleaning process. This is a sample run of
*gmodel.py*. It prints a line out each time 250 mail
messages are processed so you can see some progress happening, as this
program may run for a while processing nearly a Gigabyte of mail data.

~~~~
Loaded allsenders 1588 and mapping 28 dns mapping 1
1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
501 2006-01-12T11:17:34-05:00 lance@indiana.edu
751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
...
~~~~

The *gmodel.py* program handles a number of data cleaning
tasks.

Domain names are truncated to two levels for .com, .org, .edu, and .net.
Other domain names are truncated to three levels. So si.umich.edu
becomes umich.edu and caret.cam.ac.uk becomes cam.ac.uk. Email addresses
are also forced to lower case, and some of the @gmane.org address like
the following

~~~~
arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org
~~~~
are converted to the real address whenever there is a matching real
email address elsewhere in the message corpus.

In the *mapping.sqlite* database there are two tables
that allow you to map both domain names and individual email addresses
that change over the lifetime of the email list. For example, Steve
Githens used the following email addresses as he changed jobs over the
life of the Sakai developer list:

~~~~
s-githens@northwestern.edu
sgithens@cam.ac.uk
swgithen@mtu.edu
~~~~

We can add two entries to the Mapping table in
*mapping.sqlite* so *gmodel.py* will map
all three to one address:

~~~~
s-githens@northwestern.edu ->  swgithen@mtu.edu
sgithens@cam.ac.uk -> swgithen@mtu.edu
~~~~

You can also make similar entries in the DNSMapping table if there are
multiple DNS names you want mapped to a single DNS. The following
mapping was added to the Sakai data:

~~~~
iupui.edu -> indiana.edu
~~~~

so all the accounts from the various Indiana University campuses are
tracked together.

You can rerun the *gmodel.py* over and over as you look
at the data, and add mappings to make the data cleaner and cleaner. When
you are done, you will have a nicely indexed version of the email in
*index.sqlite*. This is the file to use to do data
analysis. With this file, data analysis will be really quick.

The first, simplest data analysis is to determine "who sent the most
mail?" and "which organization sent the most mail"? This is done using
*gbasic.py*:

~~~~
How many to dump? 5
Loaded messages= 51330 subjects= 25033 senders= 1584

Top 5 Email list participants
steve.swinsburg@gmail.com 2657
azeckoski@unicon.net 1742
ieb@tfd.co.uk 1591
csev@umich.edu 1304
david.horwitz@uct.ac.za 1184

Top 5 Email list organizations
gmail.com 7339
umich.edu 6243
uct.ac.za 2451
indiana.edu 2258
unicon.net 2055
~~~~

Observe com que rapidez o código em *gbasic.py* é executado em comparação com o código em
*gmane.py* ou mesmo com aquele em *gmodel.py*. Eles estão
todos trabalhando nos mesmos dados, mas *gbasic.py* está usando
os dados compactados e normalizados que estão armazenados em *index.sqlite*. E se
você tem muitos dados para gerenciar, um processo de várias etapas como o desse aplicativo pode demorar um pouco mais para ser desenvolvido, mas você economizará muito tempo quando você realmente começar a explorar e visualizar os seus dados.

Você pode produzir uma simples visualização da frequência de palavra nas linhas de assunto no arquivo *gword.py*:


~~~~
Range of counts: 33229 129
Output written to gword.js
~~~~

Isso produz o arquivo *gword.js* que pode ser visualizado usando o programa *gword.htm* para produzir uma nuvem de palavras semelhante a produzida no início desta seção.

Uma segunda visualização é produzida pelo código em *gline.py*. Neste código é calculado a participação de email por organizações ao longo do tempo.

~~~~
Loaded messages= 51330 subjects= 25033 senders= 1584
Top 10 Oranizations
['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
'stanford.edu', 'ox.ac.uk']
Output written to gline.js
~~~~

Sua saída é gravada no arquivo *gline.js*, que é visualizada
usando o programa *gline.htm*.

![Sakai Mail Activity by Organization](../images/mailorg)

Esta é uma aplicação relativamente complexa e sofisticada e tem
características para realizar a recuperação, limpeza e visualização de dados reais.

